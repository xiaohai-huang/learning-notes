---
sidebar_label: Regularization
title: Regularization
description: ""
---

> **Regularization** is a strategy used to build better-performing models by reducing the odds of [overfitting](020-overfitting.mdx),
> or when your model does such a good job of matching your training data that it performs **badly** on new data.
> In other words, regularization is a way to help your model generalize better by preventing it from becoming too complex.

Reduce the **magnitude** and/or **the number of parameters** to reduce model complexity.

One way to regularize is to add a constraint to the loss function:

<p>
  <center>Regularized Loss = Loss Function + Constraint</center>
</p>

There are three main types of constraints: Ridge (L2), Lasso (L1), and Elastic Net.

## References

- [Introduction to Ridge Regression - statology](https://www.statology.org/ridge-regression/)
- [Introduction to Lasso Regression - statology](https://www.statology.org/lasso-regression/)
- [The classical linear regression model is good. Why do we need regularization?](https://medium.com/@zxr.nju/the-classical-linear-regression-model-is-good-why-do-we-need-regularization-c89dba10c8eb)
- [Why Regularization Reduces Overfitting (C2W1L05)](https://www.youtube.com/watch?v=NyG-7nRpsW8)
- [Regularization (C2W1L04)](https://www.youtube.com/watch?v=6g0t3Phly2M)
